{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face ü§ó NLP Transformers pipelines with ONNX\n",
    "\n",
    "![logo](assets/logo.png)\n",
    "\n",
    "*This project is linked to the Medium blog post: [How to use Hugging Face ü§ó Transformers with ONNX in real¬†world]() (incoming)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Working environment\n",
    "\n",
    "First of all, you need to install all required dependencies. It is recommended to use and isolated environment to avoid conflicts.\n",
    "\n",
    "You can use any package manager you want. I recommend [`conda`](https://conda.io/).\n",
    "\n",
    "```bash\n",
    "conda create -y -n hf-onnx python=3.8\n",
    "```\n",
    "\n",
    "The project requires Python 3.8 or higher.\n",
    "\n",
    "All required dependencies are listed in the `requirements.txt` file. To install them, run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring colorama: markers 'platform_system == \"Windows\" and python_full_version >= \"3.6.0\" and python_version >= \"3.6\"' don't match your environment\n",
      "Ignoring pyreadline3: markers 'sys_platform == \"win32\" and python_version >= \"3.8\" and (python_version >= \"2.7\" and python_full_version < \"3.0.0\" or python_full_version >= \"3.5.0\")' don't match your environment\n",
      "Requirement already satisfied: certifi==2021.10.8 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2021.10.8)\n",
      "Collecting charset-normalizer==2.0.12\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: click==8.0.4 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (8.0.4)\n",
      "Requirement already satisfied: coloredlogs==15.0.1 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (15.0.1)\n",
      "Requirement already satisfied: filelock==3.6.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (3.6.0)\n",
      "Collecting flatbuffers==2.0\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: huggingface-hub==0.4.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.4.0)\n",
      "Requirement already satisfied: humanfriendly==10.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (10.0)\n",
      "Requirement already satisfied: idna==3.3 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (3.3)\n",
      "Collecting joblib==1.1.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: numpy==1.22.2 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (1.22.2)\n",
      "Requirement already satisfied: onnx==1.11.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (1.11.0)\n",
      "Requirement already satisfied: onnxconverter-common==1.9.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 80)) (1.9.0)\n",
      "Requirement already satisfied: onnxruntime-tools==1.7.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 83)) (1.7.0)\n",
      "Collecting onnxruntime==1.10.0\n",
      "  Using cached onnxruntime-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: packaging==21.3 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 106)) (21.3)\n",
      "Requirement already satisfied: protobuf==3.19.4 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 109)) (3.19.4)\n",
      "Collecting psutil==5.9.0\n",
      "  Using cached psutil-5.9.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Requirement already satisfied: py-cpuinfo==8.0.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 169)) (8.0.0)\n",
      "Requirement already satisfied: py3nvml==0.2.7 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 171)) (0.2.7)\n",
      "Requirement already satisfied: pyparsing==3.0.7 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 174)) (3.0.7)\n",
      "Requirement already satisfied: pyyaml==6.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 180)) (6.0)\n",
      "Requirement already satisfied: regex==2022.3.2 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 214)) (2022.3.2)\n",
      "Collecting requests==2.27.1\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: sacremoses==0.0.47 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 292)) (0.0.47)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 294)) (0.1.96)\n",
      "Requirement already satisfied: six==1.16.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 337)) (1.16.0)\n",
      "Requirement already satisfied: tf2onnx==1.8.4 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 340)) (1.8.4)\n",
      "Requirement already satisfied: tokenizers==0.11.6 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 342)) (0.11.6)\n",
      "Requirement already satisfied: torch==1.10.2 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 381)) (1.10.2+cu113)\n",
      "Requirement already satisfied: tqdm==4.63.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 401)) (4.63.0)\n",
      "Collecting transformers==4.17.0\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions==4.1.1 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 407)) (4.1.1)\n",
      "Collecting urllib3==1.26.8\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: xmltodict==0.12.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 413)) (0.12.0)\n",
      "Installing collected packages: flatbuffers, urllib3, psutil, onnxruntime, joblib, charset-normalizer, requests, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.16.2\n",
      "    Uninstalling transformers-4.16.2:\n",
      "      Successfully uninstalled transformers-4.16.2\n",
      "Successfully installed charset-normalizer-2.0.12 flatbuffers-2.0 joblib-1.1.0 onnxruntime-1.10.0 psutil-5.9.0 requests-2.27.1 transformers-4.17.0 urllib3-1.26.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üçø Export the model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we can use any TokenClassification model from Hugging Face's library because the task we are trying to solve is `Named Entity Recognition` (NER). \n",
    "\n",
    "I chose [`dslim/bert-base-NER`](https://huggingface.co/dslim/bert-base-NER) model because it is a `base` model which means medium computation time on CPU. Plus, BERT architecture is a good choice for NER.\n",
    "\n",
    "Huggging Faces's `transformers` library provides a convenient way to export the model to ONNX format. You can refer to the [official documentation](https://huggingface.co/docs/transformers/serialization#exporting-transformers-models) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `bert-base-NER` model as mentioned above and `token-classification` as feature. The `token-classification` is the task we are trying to solve. You can see the list of available by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default', 'masked-lm', 'causal-lm', 'sequence-classification', 'token-classification', 'question-answering']\n"
     ]
    }
   ],
   "source": [
    "from transformers.onnx.features import FeaturesManager\n",
    "\n",
    "distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"bert\").keys())\n",
    "print(distilbert_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By invoking the conversion script, you have to specify the model name, from a local directory or directly from the Hugging Face's hub. You also need to specify the feature as seen above. The output file will be saved in the `output` directory.\n",
    "\n",
    "We gave `onnx/` as the output directory. This is where the ONNX model will be saved.\n",
    "\n",
    "We let the `opset` parameter as default which is defined in the ONNX Config for the model. \n",
    "\n",
    "And finally, we let `atol` parameter as default which is 1e-05. This is the tolerance for the numerical precision between the original PyTorch model and the ONNX model.\n",
    "\n",
    "So here is the command to export the model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.2+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "Validating ONNX model...\n",
      "\t-[‚úì] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[‚úì] (2, 8, 9) matches (2, 8, 9)\n",
      "\t\t-[‚úì] all values close (atol: 1e-05)\n",
      "All good, model saved at: onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=dslim/bert-base-NER --feature=token-classification onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí´ Use the ONNX model with Transformers pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have exported the model to ONNX format, we can use it with the Transformers pipeline.\n",
    "\n",
    "Let's first import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from onnxruntime import (\n",
    "    InferenceSession, SessionOptions, GraphOptimizationLevel\n",
    ")\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is simple:\n",
    "- Create a session with the ONNX model that allows you to load the model into the pipeline and do inference.\n",
    "- Override the `forward` method of the pipeline to use the ONNX model.\n",
    "- Run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öôÔ∏è Create a session with the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = SessionOptions() # initialize session options\n",
    "options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "session = InferenceSession(\n",
    "    \"onnx/model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "session.disable_fallback() # disable session.run() fallback mechanism, it prevents for a reset of the execution provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use only the `CPUExecutionProvider` which is the default execution provider for the ONNX model. You can give one or more execution providers to the session. For example, you can use the `CUDAExecutionProvider` to run the model on GPU. By default, the session will use the one which is available on the machine by starting with the first one in the list.\n",
    "\n",
    "You can get the list of available execution providers by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TensorrtExecutionProvider',\n",
       " 'CUDAExecutionProvider',\n",
       " 'MIGraphXExecutionProvider',\n",
       " 'ROCMExecutionProvider',\n",
       " 'OpenVINOExecutionProvider',\n",
       " 'DnnlExecutionProvider',\n",
       " 'NupharExecutionProvider',\n",
       " 'VitisAIExecutionProvider',\n",
       " 'NnapiExecutionProvider',\n",
       " 'CoreMLExecutionProvider',\n",
       " 'ArmNNExecutionProvider',\n",
       " 'ACLExecutionProvider',\n",
       " 'DmlExecutionProvider',\n",
       " 'RknpuExecutionProvider',\n",
       " 'CPUExecutionProvider']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from onnxruntime import get_all_providers\n",
    "\n",
    "get_all_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öíÔ∏è Create a pipeline with the ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a session with the ONNX model ready to use, we can overcharge the original `TokenClassificationPipeline` class to use the ONNX model.\n",
    "\n",
    "To fully understand what is happening, you can refer to the source code of the [`TokenClassificationPipeline` class](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/pipelines/token_classification.py#L86). \n",
    "\n",
    "We will only override the `forward` and the `preprocess` methods, because the other methods are not dependent of the model format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxTokenClassificationPipeline(TokenClassificationPipeline):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    \n",
    "    def _forward(self, model_inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model. This method is not to be called by the user directly and is only used\n",
    "        by the pipeline to perform the actual predictions.\n",
    "\n",
    "        This is where we will define the actual process to do inference with the ONNX model and the session created\n",
    "        before.\n",
    "        \"\"\"\n",
    "\n",
    "        # This comes from the original implementation of the pipeline\n",
    "        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\")\n",
    "        offset_mapping = model_inputs.pop(\"offset_mapping\", None)\n",
    "        sentence = model_inputs.pop(\"sentence\")\n",
    "\n",
    "        inputs = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()} # dict of numpy arrays\n",
    "        outputs_name = session.get_outputs()[0].name # get the name of the output tensor\n",
    "\n",
    "        logits = session.run(output_names=[outputs_name], input_feed=inputs)[0] # run the session\n",
    "        logits = torch.tensor(logits) # convert to torch tensor to be compatible with the original implementation\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"special_tokens_mask\": special_tokens_mask,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"sentence\": sentence,\n",
    "            **model_inputs,\n",
    "        }\n",
    "\n",
    "    # We need to override the preprocess method because the onnx model is waiting for the attention masks as inputs\n",
    "    # along with the embeddings.\n",
    "    def preprocess(self, sentence, offset_mapping=None):\n",
    "        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False\n",
    "        model_inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_attention_mask=True, # This is the only difference from the original implementation\n",
    "            return_tensors=self.framework,\n",
    "            truncation=truncation,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_offsets_mapping=self.tokenizer.is_fast,\n",
    "        )\n",
    "        if offset_mapping:\n",
    "            model_inputs[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        model_inputs[\"sentence\"] = sentence\n",
    "\n",
    "        return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèÉ Run the pipeline\n",
    "\n",
    "We have everything set up, so we can run the pipeline.\n",
    "\n",
    "As normal, the pipeline will need a tokenizer, a model and a task. We will use the `ner` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_from_hub = \"dslim/bert-base-NER\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_from_hub)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_from_hub)\n",
    "\n",
    "onnx_pipeline = OnnxTokenClassificationPipeline(\n",
    "    task=\"ner\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    aggregation_strategy=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9978969,\n",
       "  'word': 'Apple',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9981243,\n",
       "  'word': 'Steve Jobs',\n",
       "  'start': 29,\n",
       "  'end': 39},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9741297,\n",
       "  'word': 'Steve Wozniak',\n",
       "  'start': 41,\n",
       "  'end': 54},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.99970996,\n",
       "  'word': 'Ronald Wayne',\n",
       "  'start': 59,\n",
       "  'end': 71},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.86664414,\n",
       "  'word': 'Wozniak',\n",
       "  'start': 92,\n",
       "  'end': 99},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.99852806,\n",
       "  'word': 'Apple I',\n",
       "  'start': 102,\n",
       "  'end': 109}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer\"\n",
    "\n",
    "onnx_pipeline(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here it is, the pipeline is running well with the ONNX model!** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Benchmarking a full pipeline (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will benchmark by mesuring the inference time of the pipeline with the ONNX model and the PyTorch model.\n",
    "\n",
    "We first need to load the PyTorch model and create a pipeline with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_pipeline = TokenClassificationPipeline(\n",
    "    task=\"ner\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    aggregation_strategy=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test both pipelines with the same data and 3 different sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {\n",
    "    \"short_sequence\": \"Hello my name is Thomas and I love HuggingFace.\",\n",
    "    \"medium_sequence\": \"Winston Churchill was born in 1874 in Stoke-on-Trent, England, to a German father, William and Elizabeth Churchill.\",\n",
    "    \"long_sequence\": \"\"\"The first person to reach the summit of Everest was the South Nepalese Everest Gurun, \n",
    "                who was a member of the Royal Nepal Expedition, led by the Nepalese Mountaineer, Sir Edmund Hillary. \n",
    "                Hilary lived in the Himalayas for a time. He sadly died in 1953 at the age of 88.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time the inference time for each pipeline with the 3 different sequence lengths. We will repeat each iteration 300 times for each sequence length to get a more accurate benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "results = [[\"Sequence Length\", \"PyTorch\", \"ONNX\"]]\n",
    "for k, v in sequences.items():\n",
    "    results.append(\n",
    "        [k, timeit.timeit(lambda: pytorch_pipeline(v), number=300), timeit.timeit(lambda: onnx_pipeline(v), number=300)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything in a table and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length      PyTorch      ONNX\n",
      "-----------------  ---------  --------\n",
      "short_sequence       13.6261   5.91692\n",
      "medium_sequence      16.245    7.07232\n",
      "long_sequence        30.1183  10.7552\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(results, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that looks great! üéâ\n",
    "\n",
    "Let's calcualte the ratio between the inference time of the ONNX model and the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a short sequence: ONNX is 2.30x faster than PyTorch\n",
      "For a medium sequence: ONNX is 2.30x faster than PyTorch\n",
      "For a long sequence: ONNX is 2.80x faster than PyTorch\n"
     ]
    }
   ],
   "source": [
    "print(f\"For a short sequence: ONNX is {results[1][1]/results[1][2]:.2f}x faster than PyTorch\")\n",
    "print(f\"For a medium sequence: ONNX is {results[2][1]/results[2][2]:.2f}x faster than PyTorch\")\n",
    "print(f\"For a long sequence: ONNX is {results[3][1]/results[3][2]:.2f}x faster than PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We nearly achieved a 3x speedup! üéâ"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0be00ba07bf63a0c73ab9bd0f7846fd29739e80e837fef3831f52181d58986e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('onnx-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
